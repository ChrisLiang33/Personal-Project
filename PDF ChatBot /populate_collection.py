# -*- coding: utf-8 -*-
"""Intro to Qdrant.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gGd1IMjSkJxz3XvgCFjfPEoOv4G2YTYp

# Connecting your app to qdrant with Langchain

The first thing that you need to do is create an account on qdrant cloud and subsequently create a cluster. Qdrant cloud offers a free-forever 1GB cluster for your projects.

Once you get that, the the host and API keys on your account. Then you can follow these steps.
"""

# !pip install langchain qdrant_client openai tiktoken

from langchain.vectorstores import Qdrant
from langchain.embeddings.openai import OpenAIEmbeddings
import qdrant_client
import os

# create your client

os.environ['QDRANT_HOST'] = "https://5c12d1c0-5c8c-461c-adf4-38618b9e3e47.us-east-1-0.aws.cloud.qdrant.io:6333"
os.environ['QDRANT_API_KEY'] = "aAQ-J6lwYhWO0Xi0BZM-W2DtuWCgs7IFE96M_Fcqm50vTo3w9DvruA"


client = qdrant_client.QdrantClient(
        os.getenv("QDRANT_HOST"),
        api_key=os.getenv("QDRANT_API_KEY")
    )

# create collection

os.environ['QDRANT_COLLECTION'] = "my-collection"

collection_config = qdrant_client.http.models.VectorParams(
        size=1536, # 768 for instructor-xl, 1536 for OpenAI
        distance=qdrant_client.http.models.Distance.COSINE
    )

client.recreate_collection(
    collection_name=os.getenv("QDRANT_COLLECTION"),
    vectors_config=collection_config
)

# create your vector store

os.environ['OPENAI_API_KEY'] = "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"

embeddings = OpenAIEmbeddings()

vector_store = Qdrant(
        client=client,
        collection_name=os.getenv("QDRANT_COLLECTION"),
        embeddings=embeddings
    )

# add documents to your vector database

from langchain.text_splitter import CharacterTextSplitter

# if pdf
# pdf_docs = "pdf1.pdf"
# def get_pdf_text(pdf_docs):
#     text = ""
#     for pdf in pdf_docs:
#         pdf_reader = PdfReader(pdf)
#         for page in pdf_reader.pages:
#             text += page.extract_text()
#     return text
# def get_chunks(text):
    # text_splitter = CharacterTextSplitter(
    #     separator="\n",
    #     chunk_size=1000,
    #     chunk_overlap=200,
    #     length_function=len
    # )
    # chunks = text_splitter.split_text(text)
    # return chunks
#   text = get_chunks(raw_text)
# vector_store.add_texts(text)


# if txt file
def get_chunks(text):
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks

with open("story.txt") as f:
    raw_text = f.read()

text = get_chunks(raw_text)
print(text)
vector_store.add_texts(text)

# plug the vector store to your retrieval chain

from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

qa = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=vector_store.as_retriever()
    )

query = "what is the story about?"
response = qa.run(query)

print(response)

